### 谁动了我的内存

网友的博客问题

1.http://itindex.net/detail/43004-php-%E5%86%85%E5%AD%98-%E9%97%AE%E9%A2%98

摘要

> ### 探索解决问题
>
> 1. 使用 valgrind 调试php-cgi进程
>
>    Valgrind 是一个linux常用的程序的内存调试和代码剖析，对调试C/C++程序的内存泄露很有帮助，它的机制是在系统alloc/free等函数调用上加计数。用 valgrind 调试php-cgi，要求php-cgi 是debug版本，实践证明行不通：
>
>    \1. php-cgi debug 版本放在线上根本跑不起来，性能太差 2. php程序的内存泄露，是由于一些循环引用，或者gc的逻辑错误,valgrind无法探测，它适合去检查php解释器是否有内存泄露问题
>
> 2. php解释器（Zend core）自带有检查内存泄露的机制
>
>    php解释器的核心代码叫做（Zend Core) 在用valgrind 调试php-cgi进程，我查看了php-cgi的代码，发现zend core 实现了内存泄露的自我检查 但是 同上原因，php-cgi debug 跑不起来，也无法得到调试信息
>
> 3. FreeBSD 的 DTrace
>
>    DTrace是freebsd 系统支持的核心调试器，可以在各个系统函数调用上加计数点,twitter曾经用过。这个方法最后没有使用 有如下原因：
>
>    1. 需要找一台服务器安装freebsd,并部署到线上、或者模拟负载，非常繁琐
>    2. 我仔细研究了DTrace的文档，发现这个可以认为是增强的 valgrind，也不能解决我们的问题
>
> 这3种方法都不行，陷入困境.但是换个角度思考:虽然解决php程序内存泄露没有方便的工具，但是 web 程序是按请求切分的，一个http请求，对应的php进程执行一个php文件
>
> **一个自然的想法是，记录每次 http请求处理前后php进程的内存占用率之差，然后对结果排序，就能找出，让进程内存增加可能性最大的文件 ，这个文件导致内存泄露的可能性最大**

#### 记录每次 http请求处理前后php进程的内存占用率之差，然后对结果排序，就能找出，让进程内存增加可能性最大的文件 



### 计算进程内存占用率有两种方式

```
php内置函数 memory_get_usage

这个函数是 Zend Core里面一个计数器，是zend core认为的内存使用量，但是php内存泄露有可能是zend core逻辑错误导致的，所以memory_get_usage不一定可靠

linux 系统文件 /proc/{$pid}/status 会记录某个进程的运行状态，里面的 VmRSS 字段记录了该进程使用的常驻物理内存(Residence)，这个就是该进程实际占用的物理内存了，用这个数据比较靠谱，在程序里面提取这个值也很容易

找到思路，就可以开始动手写程序

直接修改了php-cgi的源代码，在main.c里面处理每个fastcgi请求前后分别加计数代码，输出日志到log文件，重新编译上线

运行30分钟之后，执行

cat short.log| awk '{print $3 "\t" $7 "\t" $6 "\t" $4$5}' |sort -r -n |head -n 100

很容易找到最可能出现内存泄露的代码文件，然后进一步排查，重构代码，这就很简单了：能不加载的文件就不加载，大数组用完之后赶紧unset ....
```

> 后来，我才发现其实不需要去修改php的源代码，php.ini配置文件里面有两个配置项： **auto_append_file,auto_prepend_file，可以在请求前后注入代码** ....
>
> 真是悲剧
>
> web程序做性能优化也是这个思路，但是要简单很多，无需写代码，在nginx log里面加上$request_time ，用awk/sort 处理一下就可以找出瓶颈。

# 用auto_prepend_file和auto_append_file调试代码

```
auto_prepend_file = "/home/username/include/header.php" 
auto_append_file = "/home/username/include/footer.php" 
```

**1.可以在header.php文件中写日志记录访问的脚本名称来调试程序走向**

如linux下hreder.php可以写为如下

```
<?php
file_put_contents("/tmp/c",date("H:i:s").$_SERVER['REQUEST_URI']."\n",FILE_APPEND);
```



**2.可以在header.php和footer.php分别增加时间函数调用，用差值算出脚本执行时间，找出耗时程序**

header.php示例

```
<?php
$_RUN['start'] = array_sum(explode(' ',microtime()));
```

footer.php示例

```
<?php
$_RUN['end'] = array_sum(explode(' ',microtime()));
$_RUN['diff'] = ($_RUN['end']-$_RUN['start']);
if ($_RUN['diff']>3) {
        file_put_contents("/home/username/slowphp",date("H:i:s")."\t".$_RUN['diff']."\t".$_SERVER['REQUEST_URI']."\n",FILE_APPEND);
}
```

注意，nginx中多次使用 PHP_VALUE时，最后的一个会覆盖之前的。如果想设置多个配置项，需要写在一起，然后用换行分割。如：

```
fastcgi_param PHP_VALUE "auto_prepend_file=/home/www/bo56.com/header.php \n auto_append_file=/home/www/bo56.com/external/footer.php"; 
```

**如果不需要所有页面都在顶部或底部require文件，可以指定某一个文件夹内的页面文件才调用auto_prepend_file与auto_append_file**



2.“内存泄露”的特征或是基本表现

http://blog.csdn.net/tao_627/article/details/9532497

```
free
                     total       used           free     shared    buffers     cached
Mem:       4149156    4130412      18744          0      13220    2720160
-/+ buffers/cache:    1397032    2752124
Swap:      6289408        144    6289264
```

第1行

total 内存总数: 4149156
used 已经使用的内存数: 4130412
free 空闲的内存数: 18744
shared 当前已经废弃不用，总是0
buffers Buffer Cache内存数: 13220
cached Page Cache内存数: 2720160
关系：total = used + free
第2行：
-/+ buffers/cache的意思相当于：
-buffers/cache 的内存数：1397032 (等于第1行的 used - buffers - cached)
+buffers/cache 的内存数: 2752124 (等于第1行的 free + buffers + cached)

#### 可见-buffers/cache反映的是被程序实实在在吃掉的内存，而+buffers/cache反映的是可以挪用的内存总数。



令我郁闷是两项，used和cached的值太高了，而作为一个公网服务器，我还只是简单就跑了个php脚本，就把内存搞成这样，真是死了的心都用了。

```
为了提高磁盘存取效率, Linux做了一些精心的设计, 除了对dentry进行缓存(用于VFS,加速文件路径名到inode的转换), 还采取了两种主要Cache方式：Buffer Cache和Page Cache。前者针对磁盘块的读写，后者针对文件inode的读写。这些Cache有效缩短了 I/O系统调用(比如read,write,getdents)的时间。"
```

> /proc是一个虚拟文件系统,我们可以通过对它的读写操作做为与kernel实体间进行通信的一种手段.也就是说可以通过修改/proc中的文件,来对当前kernel的行为做出调整.那么我们可以通过调整/proc/sys/vm/drop_caches来释放内存.操作如下:
>
> [root@server test]# cat /proc/sys/vm/drop_caches
> 0
> 首先,/proc/sys/vm/drop_caches的值,默认为0
>
> [root@server test]# sync
>
> **手动执行sync命令**(描述:sync 命令运行 sync 子例程。如果必须停止系统，则运行 sync 命令以确保文件系统的完整性。**sync 命令将所有未写的系统缓冲区写到磁盘中，包含已修改的 i-node、已延迟的块 I/O 和读写映射文件**)
>
> [root@server test]# echo 3 > /proc/sys/vm/drop_caches
> [root@server test]# cat /proc/sys/vm/drop_caches
> 3
>
> 将/proc/sys/vm/drop_caches值设为3
>
> [root@server test]# free -m
> total used free shared buffers cached
> Mem: 249 66 182 0 0 11
> -/+ buffers/cache: 55 194
> Swap: 511 0 511
>
> 再来运行free命令,发现现在的used为66MB,free为182MB,buffers为0MB,cached为11MB.那么有效的释放了buffer和cache.

```
set_time_limit(0);  
  
function thumbnailimage($img,$width,$height,$savefile){  
$new_img = imagecreatetruecolor ( $width, $height );  
...  
imagedestroy($new_img);  
}  
//$list:是那个55万的文件名  
foreache($list as $v) {  
    $memory1=memory_get_usage();  
    file_put_contents('memory','memory1:'.$memory1."\n",FILE_APPEND);  
      
    $img = imagecreatefromjpeg($v);  
    thumbnailimage($img,480,300,$savepath);  
    imagedestroy($img);  
      
    $memory1=memory_get_usage();  
    file_put_contents('memory','memory1:'.$memory1."\n",FILE_APPEND);  
      
    system('sync && echo 3 > /proc/sys/vm/drop_caches');  
}  
```



所以网友2的结论是运行下

```
sync && echo 3 > /proc/sys/vm/drop_caches
```

### 所谓的dom内存泄露或者其他php脚本的内存泄露，其实不是真正的内存泄露，因为php进程在运行时res显示项很平稳，对于used和cached的变化，那是Linux的内存管理机制在起作用。



在linux内部将cache分为2种： 
1、write/read cache 
2、mmap 
  [http://blog.chinaunix.net/uid-26669729-id-3077015.html](http://blog.chinaunix.net/uid-26669729-id-3077015.html) 

通常一个文件被映射到内存后，就会一直占用，直到文件句柄关闭才会释放 
我们调用drop cache会释放1，但是2是释放不了的。 



1、执行sync:将所有未写的系统缓冲区写到磁盘中 
2、echo 3 > /proc/sys/vm/drop_caches:清除cache 
   1: 清除 pagecache 
   2: 清除 dentries and inodes 
   3: 清除 pagecache, dentries and inodes 



3.经验参考

```
$ php -i |grep memory
memory_limit => 1024M => 1024M //php脚本执行最大可使用内存
$php -i |grep max
max_execution_time => 0 => 0 //最大执行时间，脚本默认为0不限制，web请求默认30s
max_file_uploads => 20 => 20 //一个表单里最大上传文件数量
max_input_nesting_level => 64 => 64 //一个表单里数据最大数组深度层数
max_input_time => -1 => -1 //php从接收请求开始处理数据后的超时时间
max_input_vars => 1000 => 1000 //一个表单（包括get、post、cookie的所有数据）最多提交1000个字段
post_max_size => 8M => 8M //一次post请求最多提交8M数据
upload_max_filesize => 2M => 2M //一个可上传的文件最大不超过2M
```